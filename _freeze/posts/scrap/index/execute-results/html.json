{
  "hash": "36900790aca2e414b9c7a9c7a9564a87",
  "result": {
    "markdown": "---\ntitle: \"Scrap! o como extraer info de páginas\"\ndescription: \"No quieres copiar y pegar páginas donde están los datos y luego pasar horas limpiandolos en excel? Copia y pega este script para automatizar aunque sea un poco el proceso\"\nauthor: \"David Humberto Jiménez S.\"\ndate: \"2021-11-01\"\ndate-modified: \"2023-11-30\"\ncategories: [code, visualization, politics, spanish]\n#image: \"sismo.png\"\n---\n\n\n# ¡Holi!\n\nHe estado perdido, igual y no lo notaron, pero estoy vivo. Dos borradores de post, que están pendientes, un montón de trabajo, trámites de la escuela y demás cosas en el camino, *I'm back*.\n\nEl otro día me pidieron hacer una relación de otro proyecto en el que estaba colaborando [Análisis de las elecciones federales 2021](https://analisiselectoral2021.juridicas.unam.mx) y pensé: \"a, pues eso sale en chinga con R\", pero hace mucho no hago scraping, no pude y terminé haciéndolo a mano.\n\nPara que no les pase lo que a mí, voy a explicarles todo lo que sé esperando que no vuelvan a cambiar la *library* y lo que vean aquí no funcione (como me pasó en los 3 tutoriales de hace menos de 2 años que revisé).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'purrr' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'rvest' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(stringr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stringr' was built under R version 4.2.3\n```\n:::\n:::\n\n\nEntonces, lo primero siempre es cargar las *libraries* que vamos a utilizar. El paquete nuevo es [*rvest*](https://rvest.tidyverse.org/) que literalmente se llama así por *harvest* para \"cosechar\" los datos de una página web. Todos los paquetes de *R* son superñoños, por si no se habían dado cuenta.\n\n## Scrapping!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=\"\n\ndada <- paste0(url, \"0\") # enlace con las publicciones más recientes\n```\n:::\n\n\nComo ya revisé cuántas páginas había en ese portal, sé que van del 0 al 6. Sigo buscando un método para no tener que contarlo a mano, así que avísenme.\n\nAhora, lo que hay que hacer es entender cómo se organiza una página web. Hay personas mucho más hábiles que yo para eso, yo solo les sugiero que en la página que seleccionen den clic derecho en cualquier lado y seleccionen inspeccionar. Esto funciona si utilizas chrome.\n\nAhora, si ustedes como yo no tienen idea de páginas web, html y CSS... solo tienen que buscar lo que diga *class* y probar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitulos <- read_html(dada) %>% # Con esta función hacemos que R \"lea\" la página\n    html_nodes(\".titulo-publicacion-l\") %>% # Aquí seleccionamos el \"nodo\" o clase que queremos recuperar\n    html_text() %>% # y con esto lo convertimos a texto\n    as_tibble() %>%\n    rename(titulo = value)\n```\n:::\n\n\nEste es un vector de texto, ¿se acuerdan de las primeras entradas y como una serie de vectores acomodados crean un dataframe? Pues aquí es donde rinde sus frutos entender la diferencia. Además, el scrapping funciona por posiciones, así que si la página está lógicamente construida podemos extraer sin problemas la información necesaria\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautor <- read_html(dada) %>%\n    html_nodes(\".autores\") %>%\n    html_text() %>%\n    as_tibble() %>%\n    rename(autor = value)\n```\n:::\n\n\nEntonces los títulos ya están, los autores ya están, pero qué pasa si quiero los enlaces a las notas? Bueno, es un poco más complicado, y no es tan general, pero puedes seleccionar más de un nodo y en lugar del texto, los atributos.\n\nAdemás, no podemos olvidar las fechas. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nenlaces <- read_html(dada) %>%\n    html_nodes(\".views-field-title-1 a\") %>%\n    html_attr(\"href\") %>%\n    as_tibble() %>%\n    rename(enlaces = value)\n\nfechas <- read_html(dada) %>%\n    html_nodes(\".fuente-1 time\") %>%\n    html_text() %>%\n    as_tibble() %>%\n    rename(fecha = value)\n```\n:::\n\n\nVamos a tener todo en un olo data frame para poder consultarlo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndetalle <- \"https://analisiselectoral2021.juridicas.unam.mx\"\n\ntabla <- bind_cols(titulos, autor, enlaces) %>%\n    mutate(\n        enlaces = paste0(detalle, enlaces)\n    )\n```\n:::\n\n\nTachán!! Listo, un scrap sencillo pero poderoso. Sin embargo, hay muchas páginas. Cómo aún no me sé una manera de contarlas automáticamente, pero sé que son 7 páginas de publicaciones, pues vamos a hacer un loop, o mejor dicho un map.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlista <- map(\n    url,\n    ~ paste0(.x, seq(0, 6, by = 1))\n) %>%\n    unlist()\nlista\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=0\"\n[2] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=1\"\n[3] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=2\"\n[4] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=3\"\n[5] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=4\"\n[6] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=5\"\n[7] \"https://analisiselectoral2021.juridicas.unam.mx/publicaciones?page=6\"\n```\n:::\n:::\n\n\nEl primero que necesitamos es bastante sencillo, tenemos la dirección lista para pegarle los números. Como no la vamos a hacer a mano, hay que hacer algo. Yo opté por crearlo con un map, para que luego se convierta en lista y al momento de hacerle el *unlist* se convierte en caracteres. La otra opción sería crear un data frame con una columna de número de 0 al 6, luego pegar la columna url, pegar ambas, quitar lo que no sirve o seleccionar la nueva columna y transformarla a vector o en su defecto, utilizar *dataframe$columna*. \n\nSi se les ocurre otra manera, avísenme, que siempre se aprende algo nuevo.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitulos <- map(\n    lista,\n    ~ read_html(.x) %>%\n        html_nodes(\".titulo-publicacion-l\") %>%\n        html_text() %>%\n        as_tibble() %>%\n        rename(titulo = value)\n) %>%\n    bind_rows()\n\nautores <- map(\n    lista,\n    ~ read_html(.x) %>%\n        html_nodes(\".autores\") %>%\n        html_text() %>%\n        as_tibble() %>%\n        rename(autor = value)\n) %>%\n    bind_rows()\n\nenlaces <- map(\n    lista,\n    ~ read_html(.x) %>%\n        html_nodes(\".views-field-title-1 a\") %>%\n        html_attr(\"href\") %>%\n        as_tibble() %>%\n        rename(enlaces = value)\n) %>%\n    bind_rows()\n\nfechas <- map(\n    lista,\n    ~ read_html(.x) %>%\n        html_nodes(\".fuente-1 time\") %>%\n        html_text() %>%\n        as_tibble() %>%\n        rename(fecha = value)\n) %>%\n    bind_rows()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal <- cbind(titulos, autores, fechas, enlaces) %>%\n    mutate(\n        enlaces = paste0(detalle, enlaces)\n    )\nhead(final)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                              titulo\n1                 La forma de una tensión: democracia directa, ciudadanos y partidos\n2     La crisis del modelo de comunicación política propiciada desde la presidencia.\n3                                                                    Estudia, macho…\n4                      Aprobación de los Lineamientos para la Revocatoria de Mandato\n5                                                             El “diezmo” de Texcoco\n6 La complejidad de la paridad en la integración del Congreso de la Ciudad de México\n                      autor      fecha\n1              Nicolás Loza 2021-10-14\n2      María Marván Laborde 2021-10-11\n3              Nicolás Loza 2021-10-07\n4        Flavia Freidenberg 2021-10-05\n5 Guadalupe Salmorán Villar 2021-10-04\n6            Karolina Gilas 2021-10-04\n                                                                  enlaces\n1 https://analisiselectoral2021.juridicas.unam.mx/detalle-publicacion/157\n2 https://analisiselectoral2021.juridicas.unam.mx/detalle-publicacion/155\n3 https://analisiselectoral2021.juridicas.unam.mx/detalle-publicacion/153\n4 https://analisiselectoral2021.juridicas.unam.mx/detalle-publicacion/151\n5 https://analisiselectoral2021.juridicas.unam.mx/detalle-publicacion/149\n6 https://analisiselectoral2021.juridicas.unam.mx/detalle-publicacion/147\n```\n:::\n:::\n\n\nY listo!!! Tenemos una base de datos con los nombres de las publicaciones, autores, fechas y enlaces.\n\nPero... creo que le podemos sacar más jugo a esto... Por ejemplo, que tal si de una vez recuperamos los textos de las publicaciones? Que por qué? Por la gloria de Satán! Digo, porque podemos utilizar nuestras herramientas de análisis de texto y ver si hay cosas interesantes de esta manera, o hay que leer todo el texto para entenderlo.\n\n## Scrapping y textos\n\nEntonces, ¿cómo le hacemos? Ya tenemos la lista de enlaces\n\n\n::: {.cell}\n\n```{.r .cell-code}\npublicaciones <- map(\n    final$enlaces,\n    ~ read_html(.x) %>%\n        html_nodes(\".col-12\") %>% # Esta es la clase que buscamos\n        html_text() %>%\n        .[2] %>% # solo necesitamos el segundo elemento de la lista que arroja\n        as_tibble() %>%\n        rename(texto = value) %>%\n        mutate(enlaces = .x)\n) %>% # aunque el scrapping funciona por posiciones, a veces me da miedo y le genero identificadores para unir los DF\n    bind_rows()\n\nfinal <- left_join(final, publicaciones)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nJoining with `by = join_by(enlaces)`\n```\n:::\n:::\n\n\nY con eso, hemos leído las 70 publicaciones. Pero, hay un pequeño problema: aparece el título, tema, autor, fecha, un *disclaimer* y una leyenda para descargar un archivo. Esto porque el scrapping depende mucho de como esté armada la página. Como utilizamos una clase que agrupa todo el texto, se incluyen estos elementos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tm)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tm' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: NLP\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'ggplot2'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:NLP':\n\n    annotate\n```\n:::\n\n```{.r .cell-code}\nlibrary(syuzhet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'syuzhet' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidytext' was built under R version 4.2.3\n```\n:::\n\n```{.r .cell-code}\nlibrary(lubridate)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'lubridate'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n```\n:::\n:::\n\n\nIgual que en un par de post anteriores, [este](https://dhumbertojs.github.io/posts/2020-12-09-a-3-aos-del-sismo-desde-las-solicitudes-de-informacin/), [este](https://dhumbertojs.github.io/posts/2020-12-09-anlisis-de-texto-sentimientos/) y [este](https://dhumbertojs.github.io/posts/2020-12-09-das-sin-ti-anlisis-de-texto/), necesitamos estas dos libraries para analizar los textos.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal %>%\n    unnest_tokens(palabra, texto) %>%\n    filter(!palabra %in% stopwords(\"es\")) %>%\n    count(palabra, sort = T) %>%\n    mutate(palabra = reorder(palabra, n)) %>%\n    top_n(20) %>%\n    ggplot(aes(x = n, y = palabra)) +\n    geom_bar(stat = \"identity\") +\n    theme_classic() +\n    labs(\n        x = \"\", y = \"\",\n        title = \"Palabras más frecuentes\",\n        subtitle = \"En todas las publicaciones\"\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by n\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nComo no es ninguna sorpresa, \"INE\" y \"electoral\" son las palabras más frecuentes en todas las publicaciones. Pero, tenemos manera de revisar el top 10 de cada autor. Sin embargo, antes, vamos a ver cuantas publicaciones tiene cada autor y las publicaciones por mes, nomás porque sí.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal %>%\n    count(autor, sort = T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                         autor n\n1               Karolina Gilas 9\n2           Flavia Freidenberg 8\n3      Juan Jesús Garza Onofre 8\n4       Luz María Cruz Parcero 7\n5                 Nicolás Loza 7\n6        Roberto Lara Chagoyán 7\n7    Guadalupe Salmorán Villar 6\n8         María Marván Laborde 6\n9  Hugo Alejandro Concha Cantú 4\n10             Ximena Medellín 4\n11          Horacio Vives Segl 3\n12         Javier Martín Reyes 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal %>%\n    mutate(fecha = as_date(fecha)) %>%\n    group_by(mes = month(fecha)) %>%\n    count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 2\n# Groups:   mes [8]\n    mes     n\n  <dbl> <int>\n1     3     5\n2     4    12\n3     5     8\n4     6    16\n5     7     7\n6     8    11\n7     9     5\n8    10     6\n```\n:::\n:::\n\n\nResulta interesante que el máximo de publicaciones es 9 y el mínimo es 1. Además, el mes con mayor número de notas fue junio.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal %>%\n    unnest_tokens(palabra, texto) %>%\n    filter(!palabra %in% stopwords(\"es\")) %>%\n    count(autor, palabra, sort = T) %>%\n    group_by(autor) %>%\n    mutate(prop = n / sum(n)) %>%\n    top_n(7) %>%\n    ggplot(aes(x = prop, y = palabra, fill = autor)) +\n    facet_wrap(. ~ autor, scales = \"free_y\") +\n    geom_bar(stat = \"identity\") +\n    theme_classic() +\n    theme(legend.position = \"blank\") +\n    labs(\n        x = \"\", y = \"\",\n        title = \"Proporción de palabras más frecuentes\",\n        subtitle = \"por autor\"\n    )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSelecting by prop\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nAhora bien, la gráfica anterior era solo por palabras más frecuentes. Pero, como sabemos hay autoras que tuvieron más publicaciones. Por esta razón utilicé el top 10 de palabras más frecuentes como proporción del total de palabras de cada autora. El outlier es Javier Martín Reyes porque tiene una sola publicación. \n\nY estas son las cosas sencillas que podemos lograr con un web scrapping y las herramientas que vamos acumulando. Por ejemplo, análisis de sentimientos! No hay que olvidar que la función para esto tarda más de lo normal. Entonces no desesperen.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- final %>%\n    unnest_tokens(palabra, texto) %>%\n    filter(!palabra %in% stopwords(\"es\"))\n\ntext_nrc <- get_nrc_sentiment(char_v = text$palabra, language = \"spanish\")\n\nbind_cols(text, text_nrc) %>%\n    pivot_longer(\n        cols = 6:15\n    ) %>%\n    group_by(name) %>%\n    summarise(total = sum(value)) %>%\n    ungroup() %>%\n    mutate(name = reorder(name, total)) %>%\n    ggplot(aes(x = name, y = total, fill = name)) +\n    geom_bar(stat = \"identity\") +\n    scale_fill_manual(\n        breaks = c(\"disgust\", \"surprise\", \"anger\", \"joy\", \"anticipation\", \"fear\", \"sadness\", \"trust\", \"positive\", \"negative\"),\n        values = c(\"#24D204\", \"#06CBBE\", \"#CB0606\", \"#FBFE00\", \"#FF9200\", \"#46BF5F\", \"#5479C3\", \"#01FF4C\", \"#FF00DC\", \"#000000\")\n    ) +\n    theme_classic() +\n    labs(\n        title = \"Análisis de sentimientos en todas las publicaciones\",\n        subtitle = \"Syuzhet\",\n        x = \"\", y = \"\"\n    ) +\n    theme(legend.position = \"blank\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nTodo apunta a que las investigadoras involucradas en este proyecto confían en las decisiones tomadas por el Tribunal Electoral y el INE. Después de todo, son los organismos que deciden la política electoral en la práctica. A pesar de todo, aún hay confianza en las instituciones. Cualquier cosa mándenme un [DM](https://twitter.com/dhumbertoj)",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}